{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adc8045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a8c1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cross-Attention Module ---\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_output, mask=None):\n",
    "            # decoder_hidden: [num_queries, d_model] ← Q\n",
    "            # encoder_output: [num_keys, d_model]     ← K, V\n",
    "\n",
    "            Q = self.W_q(decoder_hidden)              # [num_queries, d_model]\n",
    "            K = self.W_k(encoder_output)              # [num_keys, d_model]\n",
    "            V = self.W_v(encoder_output)              # [num_keys, d_model]\n",
    "\n",
    "            # Compute attention scores\n",
    "            attention_scores = torch.matmul(Q, K.T)   # [num_queries, num_keys]\n",
    "\n",
    "            # Scale scores\n",
    "            d_k = K.size(-1)\n",
    "            attention_scores /= d_k ** 0.5\n",
    "\n",
    "            # Optional masking\n",
    "            if mask is not None:\n",
    "                attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "            # Softmax over encoder tokens (keys)\n",
    "            attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "            # Weighted sum of values\n",
    "            output = torch.matmul(attention_weights, V)  # [num_queries, d_model]\n",
    "\n",
    "            return output, attention_weights\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e2735b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Attention Output:\n",
      " tensor([[0.2845, 0.1918, 0.2908, 0.2556],\n",
      "        [0.2850, 0.1922, 0.2902, 0.2556]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Attention Weights:\n",
      " tensor([[0.3462, 0.3255, 0.3283],\n",
      "        [0.3487, 0.3244, 0.3269]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# --- Simulated Input ---\n",
    "\n",
    "# Suppose encoder output has 3 tokens, each with 4-dim embeddings\n",
    "encoder_output = torch.tensor([\n",
    "    [1.0, 0.5, 0.2, 0.1],\n",
    "    [0.4, 0.6, 0.9, 0.3],\n",
    "    [0.3, 0.2, 0.5, 0.7]\n",
    "])\n",
    "\n",
    "# Decoder hidden state is 2 tokens, each with 4-dim embeddings\n",
    "decoder_hidden = torch.tensor([\n",
    "    [0.9, 0.1, 0.3, 0.5],\n",
    "    [0.2, 0.8, 0.6, 0.4]\n",
    "])\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create cross-attention module\n",
    "cross_attention = CrossAttention(d_model=4)\n",
    "\n",
    "# Run cross-attention\n",
    "output, attn_weights = cross_attention.forward(decoder_hidden, encoder_output)\n",
    "\n",
    "print(\"Cross-Attention Output:\\n\", output)\n",
    "print(\"\\nAttention Weights:\\n\", attn_weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
